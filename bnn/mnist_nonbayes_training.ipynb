{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Trains a Bayesian neural network to classify MNIST digits.\n",
    "\n",
    "The architecture is LeNet-5 [1].\n",
    "\n",
    "#### References\n",
    "\n",
    "[1]: Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner.\n",
    "     Gradient-based learning applied to document recognition.\n",
    "     _Proceedings of the IEEE_, 1998.\n",
    "     http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "# Dependency imports\n",
    "from absl import app\n",
    "from absl import flags\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "from matplotlib import figure  # pylint: disable=g-import-not-at-top\n",
    "from matplotlib.backends import backend_agg\n",
    "import numpy as np\n",
    "# import tensorflow.compat.v2 as tf\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "# tf.enable_v2_behavior()\n",
    "\n",
    "# TODO(b/78137893): Integration tests currently fail with seaborn imports.\n",
    "warnings.simplefilter(action='ignore')\n",
    "\n",
    "try:\n",
    "  import seaborn as sns  # pylint: disable=g-import-not-at-top\n",
    "  HAS_SEABORN = True\n",
    "except ImportError:\n",
    "  HAS_SEABORN = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "IMAGE_SHAPE = [28, 28, 1]\n",
    "NUM_TRAIN_EXAMPLES = 60000\n",
    "NUM_HELDOUT_EXAMPLES = 10000\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "learning_rate = 0.001\n",
    "num_epochs = 50\n",
    "batch_size=128\n",
    "data_dir=os.path.join(os.getenv('TEST_TMPDIR', '/tmp'), 'bayesian_neural_network/data')\n",
    "model_dir = 'plots_mnist_mcdropout'\n",
    "viz_steps=5\n",
    "num_monte_carlo=50\n",
    "fake_data = False"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "def plot_weight_posteriors(names, qm_vals, qs_vals, fname):\n",
    "  \"\"\"Save a PNG plot with histograms of weight means and stddevs.\n",
    "\n",
    "  Args:\n",
    "    names: A Python `iterable` of `str` variable names.\n",
    "      qm_vals: A Python `iterable`, the same length as `names`,\n",
    "      whose elements are Numpy `array`s, of any shape, containing\n",
    "      posterior means of weight varibles.\n",
    "    qs_vals: A Python `iterable`, the same length as `names`,\n",
    "      whose elements are Numpy `array`s, of any shape, containing\n",
    "      posterior standard deviations of weight varibles.\n",
    "    fname: Python `str` filename to save the plot to.\n",
    "  \"\"\"\n",
    "  fig = figure.Figure(figsize=(6, 3))\n",
    "  canvas = backend_agg.FigureCanvasAgg(fig)\n",
    "\n",
    "  ax = fig.add_subplot(1, 2, 1)\n",
    "  for n, qm in zip(names, qm_vals):\n",
    "    sns.distplot(qm.reshape([-1]), ax=ax, label=n)\n",
    "  ax.set_title('weight means')\n",
    "  ax.set_xlim([-1.5, 1.5])\n",
    "  ax.legend()\n",
    "\n",
    "  ax = fig.add_subplot(1, 2, 2)\n",
    "  for n, qs in zip(names, qs_vals):\n",
    "    sns.distplot(qs.reshape([-1]), ax=ax)\n",
    "  ax.set_title('weight stddevs')\n",
    "  ax.set_xlim([0, 1.])\n",
    "\n",
    "  fig.tight_layout()\n",
    "  canvas.print_figure(fname, format='png')\n",
    "  print('saved {}'.format(fname))\n",
    "\n",
    "\n",
    "def plot_heldout_prediction(input_vals, probs,\n",
    "                            fname, n=10, title=''):\n",
    "  \"\"\"Save a PNG plot visualizing posterior uncertainty on heldout data.\n",
    "\n",
    "  Args:\n",
    "    input_vals: A `float`-like Numpy `array` of shape\n",
    "      `[num_heldout] + IMAGE_SHAPE`, containing heldout input images.\n",
    "    probs: A `float`-like Numpy array of shape `[num_monte_carlo,\n",
    "      num_heldout, num_classes]` containing Monte Carlo samples of\n",
    "      class probabilities for each heldout sample.\n",
    "    fname: Python `str` filename to save the plot to.\n",
    "    n: Python `int` number of datapoints to vizualize.\n",
    "    title: Python `str` title for the plot.\n",
    "  \"\"\"\n",
    "  fig = figure.Figure(figsize=(9, 3*n))\n",
    "  canvas = backend_agg.FigureCanvasAgg(fig)\n",
    "  for i in range(n):\n",
    "    ax = fig.add_subplot(n, 3, 3*i + 1)\n",
    "    ax.imshow(input_vals[i, :].reshape(IMAGE_SHAPE[:-1]), interpolation='None')\n",
    "\n",
    "    ax = fig.add_subplot(n, 3, 3*i + 2)\n",
    "    for prob_sample in probs:\n",
    "      sns.barplot(np.arange(10), prob_sample[i, :], alpha=0.1, ax=ax)\n",
    "      ax.set_ylim([0, 1])\n",
    "    ax.set_title('posterior samples')\n",
    "\n",
    "    ax = fig.add_subplot(n, 3, 3*i + 3)\n",
    "    sns.barplot(np.arange(10), np.mean(probs[:, i, :], axis=0), ax=ax)\n",
    "    ax.set_ylim([0, 1])\n",
    "    ax.set_title('predictive probs')\n",
    "  fig.suptitle(title)\n",
    "  fig.tight_layout()\n",
    "\n",
    "  canvas.print_figure(fname, format='png')\n",
    "  print('saved {}'.format(fname))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "\n",
    "class MNISTSequence(tf.keras.utils.Sequence):\n",
    "  \"\"\"Produces a sequence of MNIST digits with labels.\"\"\"\n",
    "\n",
    "  def __init__(self, data=None, batch_size=128, fake_data_size=None):\n",
    "    \"\"\"Initializes the sequence.\n",
    "\n",
    "    Args:\n",
    "      data: Tuple of numpy `array` instances, the first representing images and\n",
    "            the second labels.\n",
    "      batch_size: Integer, number of elements in each training batch.\n",
    "      fake_data_size: Optional integer number of fake datapoints to generate.\n",
    "    \"\"\"\n",
    "    if data:\n",
    "      images, labels = data\n",
    "    else:\n",
    "      images, labels = MNISTSequence.__generate_fake_data(\n",
    "          num_images=fake_data_size, num_classes=NUM_CLASSES)\n",
    "    self.images, self.labels = MNISTSequence.__preprocessing(\n",
    "        images, labels)\n",
    "    self.batch_size = batch_size\n",
    "\n",
    "  @staticmethod\n",
    "  def __generate_fake_data(num_images, num_classes):\n",
    "    \"\"\"Generates fake data in the shape of the MNIST dataset for unittest.\n",
    "\n",
    "    Args:\n",
    "      num_images: Integer, the number of fake images to be generated.\n",
    "      num_classes: Integer, the number of classes to be generate.\n",
    "    Returns:\n",
    "      images: Numpy `array` representing the fake image data. The\n",
    "              shape of the array will be (num_images, 28, 28).\n",
    "      labels: Numpy `array` of integers, where each entry will be\n",
    "              assigned a unique integer.\n",
    "    \"\"\"\n",
    "    images = np.random.randint(low=0, high=256,\n",
    "                               size=(num_images, IMAGE_SHAPE[0],\n",
    "                                     IMAGE_SHAPE[1]))\n",
    "    labels = np.random.randint(low=0, high=num_classes,\n",
    "                               size=num_images)\n",
    "    return images, labels\n",
    "\n",
    "  @staticmethod\n",
    "  def __preprocessing(images, labels):\n",
    "    \"\"\"Preprocesses image and labels data.\n",
    "\n",
    "    Args:\n",
    "      images: Numpy `array` representing the image data.\n",
    "      labels: Numpy `array` representing the labels data (range 0-9).\n",
    "\n",
    "    Returns:\n",
    "      images: Numpy `array` representing the image data, normalized\n",
    "              and expanded for convolutional network input.\n",
    "      labels: Numpy `array` representing the labels data (range 0-9),\n",
    "              as one-hot (categorical) values.\n",
    "    \"\"\"\n",
    "    images = 2 * (images / 255.) - 1.\n",
    "    images = images[..., tf.newaxis]\n",
    "\n",
    "    labels = tf.keras.utils.to_categorical(labels)\n",
    "    return images, labels\n",
    "\n",
    "  def __len__(self):\n",
    "    return int(tf.math.ceil(len(self.images) / self.batch_size))\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    batch_x = self.images[idx * self.batch_size: (idx + 1) * self.batch_size]\n",
    "    batch_y = self.labels[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "    return batch_x, batch_y"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Warning: deleting old log directory at plots_mnist_mcdropout\n"
     ]
    }
   ],
   "source": [
    "  if tf.io.gfile.exists(model_dir):\n",
    "    tf.compat.v1.logging.warning(\n",
    "        'Warning: deleting old log directory at {}'.format(model_dir))\n",
    "    tf.io.gfile.rmtree(model_dir)\n",
    "  tf.io.gfile.makedirs(model_dir)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "data": {
      "text/plain": "1    6742\n7    6265\n3    6131\n2    5958\n9    5949\n0    5923\n6    5918\n8    5851\n4    5842\n5    5421\ndtype: int64"
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set, heldout_set = tf.keras.datasets.mnist.load_data()\n",
    "import pandas as pd\n",
    "labels = pd.Series(train_set[1])\n",
    "labels.value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "data": {
      "text/plain": "(array([    0,     1,     2, ..., 59997, 59998, 59999], dtype=int64),)"
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "where_out = np.where(train_set[1] == 3)\n",
    "where_out\n",
    "where_in = np.where(train_set[1] != 3)\n",
    "where_in"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "data": {
      "text/plain": "(6131, 28, 28)"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_out = train_set[0][where_out]\n",
    "train_set_out = (train_set[0][where_out], train_set[1][where_out])\n",
    "train_out.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "data": {
      "text/plain": "(53869, 28, 28)"
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_in = train_set[0][where_in]\n",
    "train_set_in = (train_set[0][where_in], train_set[1][where_in])\n",
    "train_in.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "data": {
      "text/plain": "(60000, 28, 28)"
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set[0].shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "data": {
      "text/plain": "1    1135\n2    1032\n7    1028\n3    1010\n9    1009\n4     982\n0     980\n8     974\n6     958\n5     892\ndtype: int64"
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heldout_y_srs = pd.Series(heldout_set[1])\n",
    "heldout_y_srs.value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "heldout_x = heldout_set[0]\n",
    "heldout_y = heldout_set[1]\n",
    "reorder_heldout = list()\n",
    "for digit in range(10):\n",
    "    where_digit = np.where(heldout_y == digit)\n",
    "    reorder_heldout.append(where_digit[0][0])\n",
    "reorder_heldout.extend(list(range(len(heldout_x))))\n",
    "heldout_x = heldout_x[reorder_heldout]\n",
    "heldout_y = heldout_y[reorder_heldout]\n",
    "heldout_set = (heldout_x, heldout_y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "train_seq = MNISTSequence(data=train_set_in, batch_size=batch_size)\n",
    "heldout_seq = MNISTSequence(data=heldout_set, batch_size=batch_size)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "def create_model():\n",
    "  \"\"\"Creates a Keras model using the LeNet-5 architecture.\n",
    "\n",
    "  Returns:\n",
    "      model: Compiled Keras model.\n",
    "  \"\"\"\n",
    "  # KL divergence weighted by the number of training samples, using\n",
    "  # lambda function to pass as input to the kernel_divergence_fn on\n",
    "  # flipout layers.\n",
    "\n",
    "  # Define a LeNet-5 model using three convolutional (with max pooling)\n",
    "  # and two fully connected dense layers. We use the Flipout\n",
    "  # Monte Carlo estimator for these layers, which enables lower variance\n",
    "  # stochastic gradients than naive reparameterization.\n",
    "\n",
    "  model = tf.keras.models.Sequential([\n",
    "      tf.keras.layers.Convolution2D(\n",
    "          6, kernel_size=5, padding='SAME',\n",
    "          activation=tf.nn.relu),\n",
    "      tf.keras.layers.MaxPooling2D(\n",
    "          pool_size=[2, 2], strides=[2, 2],\n",
    "          padding='SAME'),\n",
    "      tf.keras.layers.Dropout(0.1),\n",
    "      tf.keras.layers.Convolution2D(\n",
    "          16, kernel_size=5, padding='SAME',\n",
    "          activation=tf.nn.relu),\n",
    "      tf.keras.layers.MaxPooling2D(\n",
    "          pool_size=[2, 2], strides=[2, 2],\n",
    "          padding='SAME'),\n",
    "      tf.keras.layers.Dropout(0.1),\n",
    "      tf.keras.layers.Convolution2D(\n",
    "          120, kernel_size=5, padding='SAME',\n",
    "          activation=tf.nn.relu),\n",
    "      tf.keras.layers.Flatten(),\n",
    "      tf.keras.layers.Dropout(0.2),\n",
    "      tf.keras.layers.Dense(\n",
    "          84,\n",
    "          activation=tf.nn.relu),\n",
    "      tf.keras.layers.Dense(\n",
    "          NUM_CLASSES,\n",
    "          activation=tf.nn.softmax)\n",
    "  ])\n",
    "\n",
    "\n",
    "  optimizer = tf.keras.optimizers.Adam(lr= learning_rate)\n",
    "  model.compile(optimizer, loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'], experimental_run_tf_function=False)\n",
    "  return model\n",
    "\n",
    "model = create_model()\n",
    "model.build(input_shape=[None, 28, 28, 1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ... Training convolutional neural network\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 300\n",
    "num_monte_carlo = 10\n",
    "print(' ... Training convolutional neural network')\n",
    "for epoch in range(num_epochs):\n",
    "    print(epoch)\n",
    "    epoch_accuracy, epoch_loss = [], []\n",
    "    for step, (batch_x, batch_y) in enumerate(train_seq):\n",
    "      batch_loss, batch_accuracy = model.train_on_batch(\n",
    "          batch_x, batch_y)\n",
    "      epoch_accuracy.append(batch_accuracy)\n",
    "      epoch_loss.append(batch_loss)\n",
    "\n",
    "      if step % 100 == 0:\n",
    "        print('Epoch: {}, Batch index: {}, '\n",
    "              'Loss: {:.3f}, Accuracy: {:.3f}'.format(\n",
    "                  epoch, step,\n",
    "                  tf.reduce_mean(epoch_loss),\n",
    "                  tf.reduce_mean(epoch_accuracy)))\n",
    "\n",
    "      if (step+1) % viz_steps == 0:\n",
    "        # Compute log prob of heldout set by averaging draws from the model:\n",
    "        # p(heldout | train) = int_model p(heldout|model) p(model|train)\n",
    "        #                   ~= 1/n * sum_{i=1}^n p(heldout | model_i)\n",
    "        # where model_i is a draw from the posterior p(model|train).\n",
    "        print(' ... Running monte carlo inference')\n",
    "        probs = tf.stack([model(heldout_x, training=True)\n",
    "                          for _ in range(num_monte_carlo)], axis=0)\n",
    "        mean_probs = tf.reduce_mean(probs, axis=0)\n",
    "        heldout_log_prob = tf.reduce_mean(tf.math.log(mean_probs))\n",
    "        print(' ... Held-out nats: {:.3f}'.format(heldout_log_prob))\n",
    "\n",
    "        if HAS_SEABORN:\n",
    "          names = [layer.name for layer in model.layers\n",
    "                   if 'flipout' in layer.name]\n",
    "          qm_vals = [layer.kernel_posterior.mean().numpy()\n",
    "                     for layer in model.layers\n",
    "                     if 'flipout' in layer.name]\n",
    "          qs_vals = [layer.kernel_posterior.stddev().numpy()\n",
    "                     for layer in model.layers\n",
    "                     if 'flipout' in layer.name]\n",
    "          plot_weight_posteriors(names, qm_vals, qs_vals,\n",
    "                                 fname=os.path.join(\n",
    "                                     model_dir,\n",
    "                                     'epoch{}_step{:05d}_weights.png'.format(\n",
    "                                         epoch, step)))\n",
    "          plot_heldout_prediction(heldout_seq.images, probs.numpy(),\n",
    "                                  fname=os.path.join(\n",
    "                                      model_dir,\n",
    "                                      'epoch{}_step{}_pred.png'.format(\n",
    "                                          epoch, step)),\n",
    "                                  title='mean heldout logprob {:.2f}'\n",
    "                                  .format(heldout_log_prob))\n",
    "model.save('mnist_nonbayes.h5')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "data": {
      "text/plain": "<keras.engine.sequential.Sequential at 0x1b44eedf9a0>"
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "num_monte_carlo = 5\n",
    "probs = tf.stack([model(heldout_x, training=True)\n",
    "                          for _ in range(num_monte_carlo)], axis=0)\n",
    "mean_probs = tf.reduce_mean(probs, axis=0)\n",
    "heldout_log_prob = tf.reduce_mean(tf.math.log(mean_probs))\n",
    "\n",
    "model_dir = 'final_nonbayes'\n",
    "plot_heldout_prediction(heldout_seq.images, probs.numpy(),\n",
    "                                  fname=os.path.join(\n",
    "                                      model_dir,\n",
    "                                      'pred.png'.format(\n",
    "                                          epoch, step)),\n",
    "                                  title='mean heldout logprob {:.2f}'\n",
    "                                  .format(heldout_log_prob))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved final_nonbayes\\pred.png\n"
     ]
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved final_nonbayes\\pred_0.png\n",
      "saved final_nonbayes\\pred_1.png\n",
      "saved final_nonbayes\\pred_2.png\n",
      "saved final_nonbayes\\pred_3.png\n",
      "saved final_nonbayes\\pred_4.png\n"
     ]
    }
   ],
   "source": [
    "heldout_x = heldout_set[0]\n",
    "heldout_y = heldout_set[1]\n",
    "reorder_heldout = list()\n",
    "\n",
    "where_digit = np.where(heldout_y == 3)[0].tolist()\n",
    "\n",
    "heldout_x = heldout_x[where_digit]\n",
    "heldout_y = heldout_y[where_digit]\n",
    "heldout_set = (heldout_x, heldout_y)\n",
    "heldout_seq = MNISTSequence(data = heldout_set, batch_size = batch_size)\n",
    "where_digit = np.where(heldout_y == 3)[0].tolist()\n",
    "num_monte_carlo = 100\n",
    "\n",
    "probs = tf.stack([model(heldout_x, training=True)\n",
    "                          for _ in range(num_monte_carlo)], axis=0)\n",
    "mean_probs = tf.reduce_mean(probs, axis=0)\n",
    "heldout_log_prob = tf.reduce_mean(tf.math.log(mean_probs))\n",
    "for i in range(5):\n",
    "    plot_heldout_prediction(heldout_seq.images[10 * i:10 * i + 10], probs.numpy()[:, 10 * i:10 * i + 10, :],\n",
    "                            fname = os.path.join(\n",
    "                                model_dir,\n",
    "                                f'pred_{i}.png'.format(\n",
    "                                    epoch, step)),\n",
    "                            title = 'mean heldout logprob {:.2f}'\n",
    "                            .format(heldout_log_prob))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}